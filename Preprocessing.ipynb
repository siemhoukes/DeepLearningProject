{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN00H70E2OFKq2bN7TDLDp+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Importing libs, connecting drive etc"],"metadata":{"id":"S-M2Hor1KIMG"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4WExAKpKBn-","executionInfo":{"status":"ok","timestamp":1715107697934,"user_tz":-120,"elapsed":5598,"user":{"displayName":"Siem","userId":"07956905929174324320"}},"outputId":"07514d62-7c53-4303-9b45-b1b5dab407e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.3\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate\n","from tensorflow.keras.models import load_model\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","print(pd.__version__)\n","\n","import matplotlib.pyplot as plt\n","pd.set_option('display.max_columns', None)"]},{"cell_type":"code","source":["### Importing the initial file if it's in the same directory as in the Github\n","df = pd.read_csv('data/PSV_merchandise_shared.csv')"],"metadata":{"id":"9CPrMvA_0EWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Importing through Google Colab\n","from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","file_path = '/content/gdrive/MyDrive/DeepLearning/Data/PSV_merchandise_shared.csv'   #add file path\n","df = pd.read_csv(file_path)"],"metadata":{"id":"6mmJf99DxjZf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Cleaning etc before feature engineering"],"metadata":{"id":"v6RKOZX3NCuQ"}},{"cell_type":"code","source":["# Removing redundant columns\n","# Columns dropped for the following reasons:\n","# 'DOB' and 'year_of_birth': Maintaining accuracy or ability to recalculate precise age is not needed.\n","# 'category_age': Can still categorize from the 'age' column if necessary.\n","# 'category_distance_from_club': Absolute distance is still retained.\n","# 'first_source_name', 'year_month_arrival', and 'year_arrival': Considered redundant.\n","# 'merchandise_purchase_channel', 'merchandise_product_description2', and 'merchandise_product_category': Redundant.\n","\n","columns_to_drop = [\n","    'Unnamed: 0',\n","    'DOB',\n","    'year_of_birth',  # Added a comma here that was missing in the original script\n","    'category_age',\n","    'category_distance_from_club',\n","    'first_source_name',\n","    'year_month_arrival',\n","    'year_arrival',\n","    'merchandise_purchase_channel',\n","    'merchandise_product_description2',\n","    'merchandise_product_category'\n","]\n","\n","df = df.drop(columns=columns_to_drop)\n","\n","# Print the DataFrame to confirm columns are dropped\n","print(df.columns)"],"metadata":{"id":"oyS2nPqMMraj","executionInfo":{"status":"ok","timestamp":1714906686805,"user_tz":-120,"elapsed":11,"user":{"displayName":"Siem","userId":"07956905929174324320"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"15120fb2-e727-482b-d127-425459898b80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['fan_id', 'age', 'gender', 'distance_from_club', 'is_fanclub_member',\n","       'is_clubcard_member', 'is_supver_member', 'is_scc_holder',\n","       'total_spend_merchandise', 'total_spend_ticket', 'total_spend_other',\n","       'total_spend_all', 'merchandise_transaction_datetime',\n","       'merchandise_product_description1', 'merchandise_product_name',\n","       'merchandise_product_size', 'merchandise_transaction_price',\n","       'merchandise_order_value', 'merchandise_product_price',\n","       'merchandise_product_units', 'merchandise_order_id'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":["### Removing instances that have multiple NA's\n","df = df.dropna(subset=[\"age\", \"gender\"])\n","\n","# Changing unknown distances to the average distance\n","valid_distances = pd.to_numeric(df['distance_from_club'], errors='coerce')  # Converts 'Unknown' to NaN\n","average_distance = int(valid_distances.mean())\n","df['distance_from_club'] = df['distance_from_club'].replace('Unknown', average_distance)\n","\n","# Getting rid of shipping costs and prints as they do not add anything to the model\n","df = df[df['merchandise_product_description1'] != 'VERZENDKOSTEN']\n","df = df[df['merchandise_product_description1'] != 'BEDRUKKING TEAMSPORT / OVERIG']\n","df = df[df['merchandise_product_description1'] != 'BEDRUKKING PSV']\n","\n","# Changing gender to a binary value\n","df.loc[:, 'gender'] = df['gender'].replace({'M': 0, 'F': 1})"],"metadata":{"id":"rj0_UzIiKRA5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature engineering"],"metadata":{"id":"aUvil-YROMH9"}},{"cell_type":"markdown","source":["### Removing returned products and corresponding purchases"],"metadata":{"id":"Sv6-hjdiXews"}},{"cell_type":"code","source":["# Add match_key in the original dataframe for all transactions\n","df['match_key'] = (df['fan_id'].astype(str) + '-' +\n","                   df['merchandise_product_name'] + '-' +\n","                   df['merchandise_product_size'] + '-' +\n","                   df['merchandise_transaction_price'].abs().astype(str))\n","\n","# Separate returns and purchases\n","returns = df[df['merchandise_product_units'] < 0]\n","purchases = df[df['merchandise_product_units'] > 0]\n","\n","# Count occurrences of each match_key in purchases and returns\n","purchase_counts = purchases['match_key'].value_counts()\n","return_counts = returns['match_key'].value_counts()\n","\n","# Create a DataFrame from purchase_counts and return_counts to handle multiple returns/purchases\n","counts_df = pd.DataFrame({\n","    'purchases': purchase_counts,\n","    'returns': return_counts\n","}).fillna(0)  # Fill NaN with 0 where there are no purchases or returns\n","\n","# The minimum of purchase and return counts is how many can be matched\n","counts_df['matched'] = counts_df[['purchases', 'returns']].min(axis=1)\n","\n","# Apply matching to the original DataFrame to flag returns\n","def flag_returns(row):\n","    if row['merchandise_product_units'] < 0 and counts_df.at[row['match_key'], 'matched'] > 0:\n","        counts_df.at[row['match_key'], 'matched'] -= 1\n","        return True\n","    return False\n","\n","df['is_returned'] = df.apply(flag_returns, axis=1)\n","\n","# Amount of returns\n","print(\"Flagged returns:\", df['is_returned'].sum())\n","\n","# Removing all flagged returns\n","df = df[~df['is_returned']]\n","\n","# Removing all remaining return transactions\n","df = df[df['merchandise_product_units'] > 0]\n","\n","# Optional: print the shape of the dataframe to see the number of rows after cleaning\n","print(\"DataFrame shape after removing flagged and all returns:\", df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AgiSeh7MOP4m","executionInfo":{"status":"ok","timestamp":1714906720839,"user_tz":-120,"elapsed":3269,"user":{"displayName":"Siem","userId":"07956905929174324320"}},"outputId":"42ab1fac-c745-410b-834c-8f2b6a05da7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Flagged returns: 9635\n","DataFrame shape after removing flagged and all returns: (186156, 23)\n"]}]},{"cell_type":"markdown","source":["### Adding a kid size column"],"metadata":{"id":"Ufj6Ub8ZXpdE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5LWq_54oMWB"},"outputs":[],"source":["### Kid Size check\n","# Define the list of child sizes\n","child_sizes = [62, 68, 74, 80, 88, 92, 98, 104, 110, 128, 140, 152, 164, 176]\n","\n","# Function to check if the size or product category indicates a kid size\n","def check_if_child_size(row):\n","    try:\n","        # Convert to integer if possible because child sizes are numbers\n","        size = int(row['merchandise_product_size'])\n","        if size in child_sizes:\n","            return 1\n","    except ValueError:\n","        # If conversion to int fails, continue to check the product category\n","        pass\n","\n","    # Check the merchandise product category\n","    if 'BABY' in row['merchandise_product_description1']:\n","        return 1\n","    else:\n","        return 0\n","\n","# Apply the function to each row of the DataFrame\n","df['is_kid_size'] = df.apply(check_if_child_size, axis=1)\n"]},{"cell_type":"markdown","source":["### Remove additional columns that were necessary for Feature Engineering"],"metadata":{"id":"ex0hBY0vkPMx"}},{"cell_type":"code","source":["# Removing columns used for feature engineering after their usage\n","# These columns were initially retained for important processing steps\n","# and are now being dropped as they are no longer needed.\n","\n","columns_to_drop = [\n","    'merchandise_order_id',\n","    'merchandise_product_size',\n","    'merchandise_transaction_datetime',\n","    'match_key',\n","    'is_returned',\n","    'merchandise_transaction_price',\n","    'merchandise_order_value',\n","    'merchandise_product_units'\n","]\n","\n","df = df.drop(columns=columns_to_drop)"],"metadata":{"id":"J5J1RCfpkNSs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Removing duplicate orders, as they might influence the model too greatly without making it more efficient.\n","# Number of rows before removing duplicates: 186156\n","# Number of rows after removing duplicates: 180661\n","\n","df = df.drop_duplicates(keep='first')\n"],"metadata":{"id":"PA7sxcEUaqyc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Adding negative instances"],"metadata":{"id":"Zdla3UIrWQ_m"}},{"cell_type":"code","source":["# Step 1: Generate negative samples with full user and product data\n","negative_samples = []\n","\n","# Get all unique products and users based on your specified columns\n","all_products = df['merchandise_product_name'].unique()\n","all_users = df['fan_id'].unique()\n","\n","# Loop over each user\n","for fan_id in all_users:\n","    user_data = df[df['fan_id'] == fan_id].iloc[0]  # Get the user's data from any of their transactions\n","\n","    # Find all products this user has purchased\n","    purchased_products = df[df['fan_id'] == fan_id]['merchandise_product_name'].unique()\n","\n","    # Identify products not purchased by this user\n","    non_purchased_products = np.setdiff1d(all_products, purchased_products)\n","\n","    # Randomly select non-purchased products to create negative samples\n","    num_neg_samples = 3 * len(purchased_products)  # Matching number of positive samples\n","    sampled_non_purchased_products = np.random.choice(non_purchased_products, size=num_neg_samples, replace=False)\n","\n","    # Append negative samples with user data and product data\n","    for product_name in sampled_non_purchased_products:\n","        product_data = df[df['merchandise_product_name'] == product_name].iloc[0]  # Get product data from any transaction involving this product\n","\n","        # Combine user and product data, overwrite product-specific fields with non-purchased product data\n","        combined_data = user_data.copy()\n","        for field in product_features:  # Assuming product_features list is defined with product-specific field names\n","            combined_data[field] = product_data[field]\n","\n","        # Set interaction to 0\n","        combined_data['interaction'] = 0\n","\n","        # Append to negative_samples list\n","        negative_samples.append(combined_data)\n","\n","# Convert negative_samples to DataFrame\n","negative_samples_df = pd.DataFrame(negative_samples)\n","\n","# Step 2: Combine the original data with the negative samples\n","df_full = pd.concat([df, negative_samples_df], ignore_index=True)\n","\n","# Step 3: Shuffle the DataFrame to mix positive and negative samples\n","df_full = df_full.sample(frac=1).reset_index(drop=True)"],"metadata":{"id":"K3Bh1K8yWUB6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exporting"],"metadata":{"id":"J9k4HCfBmisY"}},{"cell_type":"code","source":["### export CSV from cleaned file\n","\n","df_full.to_csv('/content/gdrive/MyDrive/DeepLearning/Data/psv_processed.csv', index=False)"],"metadata":{"id":"dG2x3leWhWcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Temp\n","\n","display(df[:])"],"metadata":{"id":"a8zyF_NSYqJk"},"execution_count":null,"outputs":[]}]}