{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOkT02STyvtjYQ5JMiGm1di"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Initial steps"],"metadata":{"id":"HUJeFVlBgJb8"}},{"cell_type":"markdown","source":["### Libraries"],"metadata":{"id":"DheqmA4aglqJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"78DPIGjUf3J9"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate, Dropout\n","from tensorflow.keras.models import load_model\n","from sklearn.model_selection import train_test_split, KFold\n","from google.cloud import storage\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n","from tensorflow.keras.utils import normalize\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.callbacks import EarlyStopping\n","import os\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["### Importing DataFrame"],"metadata":{"id":"D83--q_wgoBW"}},{"cell_type":"code","source":["### Importing the file if it's in the same directory as in the Github\n","df = pd.read_csv('data/psv_processed.csv')"],"metadata":{"id":"89BjZESnz58E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Importing the file through google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","\n","# File zonder verzendkosten, purchases die returned zijn en returns\n","file_path = '/content/gdrive/MyDrive/DeepLearning/Data/psv_processed.csv'   #add file path\n","df = pd.read_csv(file_path)\n"],"metadata":{"id":"Ux-DQBO6gN7G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Importing the file through GCE bucket\n","client = storage.Client()\n","bucket = client.bucket('deeplearningbucket123')\n","\n","blob = bucket.blob('three_negative.csv')\n","blob.download_to_filename('three_negative.csv')\n","df = pd.read_csv('three_negative.csv')\n"],"metadata":{"id":"p1o3ib34gv6O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preparing the model"],"metadata":{"id":"VgQMF2tcf-Iz"}},{"cell_type":"markdown","source":["### Word2Vec & PCA"],"metadata":{"id":"zjPSyZEOkoYS"}},{"cell_type":"code","source":["# The CSV could not be imported correctly with word2vec included so has to be done here\n","# Prepare the product names data\n","product_names = df['merchandise_product_name'].apply(lambda x: preprocess_string(x))\n","\n","# Train Word2Vec model\n","w2vmodel = Word2Vec(sentences=product_names, vector_size=100, window=5, min_count=3, workers=2, sg=1, epochs=20)\n","\n","# Function to convert product name to vector\n","def product_name_to_vector(product_name):\n","    words = preprocess_string(product_name)\n","    word_vectors = [w2vmodel.wv[word] for word in words if word in w2vmodel.wv]\n","    if not word_vectors:\n","        return np.zeros(w2vmodel.vector_size)  # Return a zero vector if no words are found\n","    return np.mean(np.array(word_vectors), axis=0)\n","\n","# Apply function and ensure type is np.array\n","df['product_vector'] = df['merchandise_product_name'].apply(product_name_to_vector).apply(np.array)"],"metadata":{"id":"ZgdzmV2fiMCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instantiate PCA with 1 component\n","pca = PCA(n_components=1)\n","\n","# Apply PCA transformation to product vectors\n","df['product_pca'] = pca.fit_transform(df['product_vector'].to_list())"],"metadata":{"id":"n7PeWXeVk5rx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Encoding and Normalizing"],"metadata":{"id":"ztS0ony_kpvR"}},{"cell_type":"code","source":["label_encoders = {}\n","\n","# Features requiring encoding\n","encoding_features = ['fan_id', 'merchandise_product_name'] + ['gender', 'is_fanclub_member', 'is_clubcard_member', 'is_supver_member', 'is_scc_holder', 'merchandise_product_description1', 'is_kid_size']\n","\n","# Fit and transform each categorical feature\n","for feature in encoding_features:\n","    le = LabelEncoder()\n","    df[feature] = le.fit_transform(df[feature])\n","    label_encoders[feature] = le\n","\n","numerical_features = ['age', 'distance_from_club', 'total_spend_merchandise', 'total_spend_ticket', 'total_spend_other', 'total_spend_all', 'merchandise_product_price', 'product_pca']\n","scaler = StandardScaler()\n","df[numerical_features] = scaler.fit_transform(df[numerical_features])"],"metadata":{"id":"UhNAARYfhojt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### The model itself, including 5-fold cross validation validation"],"metadata":{"id":"pKWoGS3fjcxX"}},{"cell_type":"code","source":["# Early stopping\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',\n","    patience=25,\n","    restore_best_weights=True\n",")\n","\n","# Define the maximum number of features for embedding layers\n","num_users = df['fan_id'].nunique()\n","num_products = df['merchandise_product_name'].nunique()\n","\n","# Function to build the model with editable hyperparameters, including learning rate\n","def build_model(embedding_size, l2_regularization, dropout_rate, layers, learning_rate):\n","    user_input = Input(shape=(1,), name='user_input')\n","    product_input = Input(shape=(1,), name='product_input')\n","    user_additional_features = Input(shape=(len(user_features) - 1,), name='user_additional_features')\n","    product_additional_features = Input(shape=(len(product_features) - 1,), name='product_additional_features')\n","\n","    user_embedding = Embedding(num_users, embedding_size, name='user_embedding')(user_input)\n","    product_embedding = Embedding(num_products, embedding_size, name='product_embedding')(product_input)\n","\n","    user_vec = Flatten(name='flatten_user')(user_embedding)\n","    product_vec = Flatten(name='flatten_product')(product_embedding)\n","\n","    combined_features = Concatenate()([user_vec, product_vec, user_additional_features, product_additional_features])\n","\n","    # Dynamically create the layers according to the input `layers` list\n","    x = combined_features\n","    for layer_size in layers:\n","        x = Dense(layer_size, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_regularization))(x)\n","        x = Dropout(dropout_rate)(x)\n","\n","    output = Dense(1, activation='sigmoid')(x)\n","\n","    # Set the learning rate in the Adam optimizer\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","    model = Model(inputs=[user_input, product_input, user_additional_features, product_additional_features], outputs=output)\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# Define hyperparameter combinations (as a list of dictionaries), including the learning rate\n","hyperparameters = [\n","    {'layers': [128, 64, 32], 'embedding_size': 50, 'l2_regularization': 0.04, 'dropout_rate': 0.2, 'learning_rate': 0.0001}\n","]\n","\n","# Data preparation\n","user_features = [\n","    'fan_id', 'age', 'gender', 'distance_from_club', 'is_fanclub_member',\n","    'is_clubcard_member', 'is_supver_member', 'is_scc_holder',\n","    'total_spend_merchandise', 'total_spend_ticket', 'total_spend_other', 'total_spend_all'\n","]\n","product_features = [\n","    'merchandise_product_name', 'merchandise_product_description1',\n","    'merchandise_product_price', 'is_kid_size', 'product_pca'\n","]\n","\n","# Cross-validation loop\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Loop through each set of hyperparameters\n","for params in hyperparameters:\n","    layers = params['layers']\n","    embedding_size = params['embedding_size']\n","    l2_regularization = params['l2_regularization']\n","    dropout_rate = params['dropout_rate']\n","    learning_rate = params['learning_rate']\n","\n","    fold = 1\n","    all_fold_histories = []\n","    for train_index, test_index in kf.split(df):\n","        train, test = df.iloc[train_index], df.iloc[test_index]\n","\n","        train_inputs = [\n","            train['fan_id'].values.astype('int32'),  # Ensure integer type for ID inputs\n","            train['merchandise_product_name'].values.astype('int32'),\n","            train[user_features[1:]].values,  # excluding 'fan_id' which is already used in embedding\n","            train[product_features[1:]].values  # excluding 'merchandise_product_name'\n","        ]\n","\n","        test_inputs = [\n","            test['fan_id'].values.astype('int32'),\n","            test['merchandise_product_name'].values.astype('int32'),\n","            test[user_features[1:]].values,  # excluding 'fan_id'\n","            test[product_features[1:]].values  # excluding 'merchandise_product_name'\n","        ]\n","\n","        train_labels = train['interaction'].values.astype('float32')\n","        test_labels = test['interaction'].values.astype('float32')\n","\n","        # Build a new model for each fold using the current hyperparameters\n","        model = build_model(embedding_size, l2_regularization, dropout_rate, layers, learning_rate)\n","\n","        # Train the model\n","        history = model.fit(\n","            train_inputs,\n","            train_labels,\n","            batch_size=256,\n","            epochs=25,\n","            validation_data=(test_inputs, test_labels),\n","            callbacks=[early_stopping]\n","        )\n","\n","        print(f'Finished fold {fold} with layers {layers}, embedding size {embedding_size}, L2 regularization {l2_regularization}, dropout rate {dropout_rate}, and learning rate {learning_rate}.')\n","        fold += 1\n","\n","model.save(\"psv_model\")"],"metadata":{"id":"5HtYXDgDj-N8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Validation\n"],"metadata":{"id":"56cQ03VIkLcc"}},{"cell_type":"code","source":["# Function to plot confusion matrix\n","def plot_confusion_matrix(model, test_inputs, test_labels):\n","    predictions = model.predict(test_inputs)\n","    predictions = (predictions > 0.5).astype(int)\n","    cm = confusion_matrix(test_labels, predictions)\n","\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n","\n","# Function to plot precision-recall curve\n","def plot_precision_recall_curve(model, test_inputs, test_labels):\n","    predictions = model.predict(test_inputs).ravel()\n","    precision, recall, _ = precision_recall_curve(test_labels, predictions)\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(recall, precision)\n","    plt.xlabel('Recall')\n","    plt.ylabel('Precision')\n","    plt.title('Precision-Recall Curve')\n","    plt.show()\n","\n","def evaluate_model_performance(model, test_inputs, test_labels):\n","    # Generate predictions\n","    predictions = model.predict(test_inputs).ravel()\n","    predictions_binary = (predictions > 0.5).astype(int)\n","\n","    # Calculate metrics\n","    precision = precision_score(test_labels, predictions_binary)\n","    recall = recall_score(test_labels, predictions_binary)\n","    f1 = f1_score(test_labels, predictions_binary)\n","    roc_auc = roc_auc_score(test_labels, predictions)\n","    pr_auc = average_precision_score(test_labels, predictions)\n","\n","def evaluate_model_performance(model, test_inputs, test_labels):\n","    # Generate predictions\n","    predictions = model.predict(test_inputs).ravel()\n","    predictions_binary = (predictions > 0.5).astype(int)\n","\n","    # Calculate metrics\n","    precision = precision_score(test_labels, predictions_binary)\n","    recall = recall_score(test_labels, predictions_binary)\n","    f1 = f1_score(test_labels, predictions_binary)\n","    roc_auc = roc_auc_score(test_labels, predictions)\n","    pr_auc = average_precision_score(test_labels, predictions)\n","\n","    # Display the metrics\n","    print(f'Precision: {precision:.2f}')\n","    print(f'Recall: {recall:.2f}')\n","    print(f'F1 Score: {f1:.2f}')\n","    print(f'ROC AUC: {roc_auc:.2f}')\n","    print(f'Precision-Recall AUC: {pr_auc:.2f}')\n","\n","    return {\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1,\n","        'roc_auc': roc_auc,\n","        'pr_auc': pr_auc\n","    }\n"],"metadata":{"id":"YSAoEuEYqgGo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the graphs\n","print(evaluate_model_performance(model, test_inputs, test_labels))\n","plot_confusion_matrix(model, test_inputs, test_labels)\n","plot_roc_curve(model, test_inputs, test_labels)\n","plot_precision_recall_curve(model, test_inputs, test_labels)\n"],"metadata":{"id":"tGx6KYe7q7Cw"},"execution_count":null,"outputs":[]}]}